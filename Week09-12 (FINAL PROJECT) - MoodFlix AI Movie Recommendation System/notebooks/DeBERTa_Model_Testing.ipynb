{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c85adec002ad403b9a5128b012a7ba8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_269468c820cd4cdc828eb8404bfa5177",
              "IPY_MODEL_29fa4dc0e6544c4d8208638b73b20260",
              "IPY_MODEL_17ce959e4c004923b9783d6b7f97b7f0"
            ],
            "layout": "IPY_MODEL_09aa24ef73384b569c8e35aec4d3a0a3"
          }
        },
        "269468c820cd4cdc828eb8404bfa5177": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5ef9690541e4678852beda28fa8ef00",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a92de67fe3da4e33b91232477b45660a",
            "value": "Testing:‚Äá100%"
          }
        },
        "29fa4dc0e6544c4d8208638b73b20260": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8daaa7a773f2442caf297268dc61260f",
            "max": 149,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85a17b98fe25460fa7768c37679f1eb3",
            "value": 149
          }
        },
        "17ce959e4c004923b9783d6b7f97b7f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad7bf2ea7eac40a9a1dca8ac28264e21",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f41d6ee325cb4143bda406e23b0fca1c",
            "value": "‚Äá149/149‚Äá[00:47&lt;00:00,‚Äá‚Äá3.19batch/s,‚Äáloss=0.2549]"
          }
        },
        "09aa24ef73384b569c8e35aec4d3a0a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5ef9690541e4678852beda28fa8ef00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a92de67fe3da4e33b91232477b45660a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8daaa7a773f2442caf297268dc61260f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85a17b98fe25460fa7768c37679f1eb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "blue",
            "description_width": ""
          }
        },
        "ad7bf2ea7eac40a9a1dca8ac28264e21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f41d6ee325cb4143bda406e23b0fca1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c85adec002ad403b9a5128b012a7ba8b",
            "269468c820cd4cdc828eb8404bfa5177",
            "29fa4dc0e6544c4d8208638b73b20260",
            "17ce959e4c004923b9783d6b7f97b7f0",
            "09aa24ef73384b569c8e35aec4d3a0a3",
            "d5ef9690541e4678852beda28fa8ef00",
            "a92de67fe3da4e33b91232477b45660a",
            "8daaa7a773f2442caf297268dc61260f",
            "85a17b98fe25460fa7768c37679f1eb3",
            "ad7bf2ea7eac40a9a1dca8ac28264e21",
            "f41d6ee325cb4143bda406e23b0fca1c"
          ]
        },
        "id": "_3RzR1HWrO6x",
        "outputId": "8d730749-c3af-42c3-c41c-8bf3d1262033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "                    üî¨ DeBERTa v3 Comprehensive Model Testing üî¨\n",
            "                         SuperEmotion - 7 Emotions\n",
            "==========================================================================================\n",
            "\n",
            "üñ•Ô∏è  Device: cuda\n",
            "   GPU: Tesla T4\n",
            "   Memory: 14.74 GB\n",
            "\n",
            "üîó Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "‚úÖ Drive mounted!\n",
            "üìÅ Results will be saved to: /content/drive/MyDrive/ModelTestResults/test_run_20251114_233004\n",
            "\n",
            "==========================================================================================\n",
            "üöÄ STARTING COMPREHENSIVE MODEL TESTING\n",
            "==========================================================================================\n",
            "\n",
            "==========================================================================================\n",
            "STEP 1: LOADING METADATA\n",
            "==========================================================================================\n",
            "üìÇ Loading from: /content/drive/MyDrive/SuperEmotion/metadata.json\n",
            "‚úÖ Metadata loaded!\n",
            "   Emotions: anger, fear, joy, love, neutral, sadness, surprise\n",
            "   Classes: 7\n",
            "\n",
            "==========================================================================================\n",
            "STEP 2: LOADING MODEL\n",
            "==========================================================================================\n",
            "‚úÖ All required files found in: /content/drive/MyDrive/BestModelSave/best_model/\n",
            "\n",
            "üìÇ Loading config...\n",
            "‚úÖ Config loaded\n",
            "   Hidden size: 768\n",
            "   Num layers: 12\n",
            "   Num heads: 12\n",
            "\n",
            "ü§ñ Initializing model architecture...\n",
            "\n",
            "üìÇ Loading DeBERTa weights from safetensors...\n",
            "‚úÖ DeBERTa weights loaded successfully\n",
            "\n",
            "üìÇ Loading classifier head...\n",
            "‚úÖ Classifier head loaded successfully\n",
            "\n",
            "‚úÖ Model ready!\n",
            "   Total parameters: 183,836,935\n",
            "   Trainable parameters: 183,836,935\n",
            "   Model size: ~701.28 MB\n",
            "   Device: cuda\n",
            "\n",
            "üìä Training metrics found:\n",
            "   Val Accuracy: 0.9105\n",
            "   Val F1 Macro: 0.9050\n",
            "\n",
            "==========================================================================================\n",
            "STEP 3: LOADING TEST DATA\n",
            "==========================================================================================\n",
            "üìÇ Loading from: /content/drive/MyDrive/SuperEmotion/tokenized_test.pt\n",
            "‚úÖ Test data loaded!\n",
            "   Samples: 19,072\n",
            "   Max length: 128\n",
            "‚úÖ DataLoader created!\n",
            "   Batches: 149\n",
            "   Batch size: 128\n",
            "\n",
            "==========================================================================================\n",
            "STEP 4: EVALUATING MODEL\n",
            "==========================================================================================\n",
            "\n",
            "üîç Running inference on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Testing:   0%|          | 0/149 [00:00<?, ?batch/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c85adec002ad403b9a5128b012a7ba8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Inference complete!\n",
            "   Total samples: 19,072\n",
            "   Average loss: 0.2549\n",
            "\n",
            "üìä Computing metrics...\n",
            "‚úÖ All metrics computed!\n",
            "\n",
            "==========================================================================================\n",
            "TEST RESULTS\n",
            "==========================================================================================\n",
            "\n",
            "üìä Overall Metrics:\n",
            "   Loss:               0.2549\n",
            "   Accuracy:           0.9054\n",
            "   Balanced Accuracy:  0.9022\n",
            "   F1 Macro:           0.8992\n",
            "   F1 Weighted:        0.9064\n",
            "   F1 Micro:           0.9054\n",
            "   Precision Macro:    0.8996\n",
            "   Precision Weighted: 0.9107\n",
            "   Recall Macro:       0.9022\n",
            "   Recall Weighted:    0.9054\n",
            "   MCC:                0.8897\n",
            "   Cohen's Kappa:      0.8892\n",
            "   AUC-ROC Macro:      0.9937\n",
            "   AUC-ROC Weighted:   0.9942\n",
            "\n",
            "üìä Per-Class Metrics:\n",
            "Emotion          F1  Precision   Recall  AUC-ROC\n",
            "--------------------------------------------------\n",
            "anger        0.9195     0.8814   0.9610   0.9963\n",
            "fear         0.9045     0.9538   0.8600   0.9959\n",
            "joy          0.9401     0.9933   0.8923   0.9949\n",
            "love         0.9261     0.9070   0.9460   0.9952\n",
            "neutral      0.8402     0.8086   0.8744   0.9883\n",
            "sadness      0.9409     0.9709   0.9127   0.9967\n",
            "surprise     0.8233     0.7821   0.8691   0.9885\n",
            "\n",
            "üìä Confusion Matrix:\n",
            "[[2883   22    4    3   71    8    9]\n",
            " [ 149 2580    2    2   28    5  234]\n",
            " [  25    4 2677  217   44    5   28]\n",
            " [   5    0    2 2838  113   11   31]\n",
            " [ 103   26    8   51 2138   41   78]\n",
            " [  84   68    1    4   91 2738   14]\n",
            " [  22    5    1   14  159   12 1414]]\n",
            "\n",
            "==========================================================================================\n",
            "STEP 5: SAVING RESULTS\n",
            "==========================================================================================\n",
            "‚úÖ Metrics saved: /content/drive/MyDrive/ModelTestResults/test_run_20251114_233004/test_metrics.json\n",
            "‚úÖ CSV saved: /content/drive/MyDrive/ModelTestResults/test_run_20251114_233004/per_class_metrics.csv\n",
            "\n",
            "üìä Creating visualizations...\n",
            "‚úÖ All visualizations saved!\n",
            "\n",
            "üìä Creating train/test comparison...\n",
            "‚úÖ Comparison plot saved!\n",
            "‚úÖ Summary report saved: /content/drive/MyDrive/ModelTestResults/test_run_20251114_233004/test_report.txt\n",
            "\n",
            "‚úÖ Predictions saved: /content/drive/MyDrive/ModelTestResults/test_run_20251114_233004/predictions.csv\n",
            "\n",
            "==========================================================================================\n",
            "üéâ TESTING COMPLETE!\n",
            "==========================================================================================\n",
            "\n",
            "‚è±Ô∏è  Total time: 99.94s (1.67 minutes)\n",
            "\n",
            "üìä Key Results:\n",
            "   ‚Ä¢ Accuracy: 0.9054\n",
            "   ‚Ä¢ F1 Macro: 0.8992\n",
            "   ‚Ä¢ AUC-ROC:  0.9937\n",
            "\n",
            "üìÅ All results saved to: /content/drive/MyDrive/ModelTestResults/test_run_20251114_233004\n",
            "\n",
            "üìÑ Generated files:\n",
            "   ‚Ä¢ test_metrics.json - All metrics in JSON format\n",
            "   ‚Ä¢ per_class_metrics.csv - Detailed per-class metrics\n",
            "   ‚Ä¢ predictions.csv - All predictions with probabilities\n",
            "   ‚Ä¢ test_report.txt - Comprehensive text report\n",
            "   ‚Ä¢ confusion_matrices.png - Raw and normalized confusion matrices\n",
            "   ‚Ä¢ per_class_metrics.png - Per-class performance visualization\n",
            "   ‚Ä¢ overall_metrics.png - Overall model performance\n",
            "   ‚Ä¢ roc_curves.png - ROC curves for each class\n",
            "   ‚Ä¢ precision_recall_curves.png - PR curves for each class\n",
            "   ‚Ä¢ error_analysis.png - Misclassification patterns\n",
            "   ‚Ä¢ class_accuracies.png - Per-class accuracy breakdown\n",
            "   ‚Ä¢ train_test_comparison.png - Validation vs Test comparison\n",
            "\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "DeBERTa v3 Comprehensive Model Testing Script\n",
        "Tests saved model on test set with extensive metrics and visualizations\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import DebertaV2Model, DebertaV2Config\n",
        "from sklearn.metrics import (\n",
        "    f1_score, precision_score, recall_score, accuracy_score,\n",
        "    confusion_matrix, classification_report, roc_auc_score,\n",
        "    matthews_corrcoef, cohen_kappa_score, balanced_accuracy_score,\n",
        "    roc_curve, auc, precision_recall_curve, average_precision_score\n",
        ")\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import safetensors.torch\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==================== CONFIGURATION ====================\n",
        "DATA_DIR = '/content/drive/MyDrive/SuperEmotion/'\n",
        "MODEL_DIR = '/content/drive/MyDrive/BestModelSave/best_model/'\n",
        "RESULTS_DIR = '/content/drive/MyDrive/ModelTestResults/'\n",
        "TEST_DATA_PATH = '/content/drive/MyDrive/SuperEmotion/tokenized_test.pt'\n",
        "METADATA_PATH = '/content/drive/MyDrive/SuperEmotion/metadata.json'\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "USE_MIXED_PRECISION = True\n",
        "\n",
        "print(\"=\"*90)\n",
        "print(\" \"*20 + \"üî¨ DeBERTa v3 Comprehensive Model Testing üî¨\")\n",
        "print(\" \"*25 + \"SuperEmotion - 7 Emotions\")\n",
        "print(\"=\"*90)\n",
        "print(f\"\\nüñ•Ô∏è  Device: {DEVICE}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "# ==================== MOUNT DRIVE ====================\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        print(\"\\nüîó Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"‚úÖ Drive mounted!\")\n",
        "    else:\n",
        "        print(\"\\n‚úÖ Drive already mounted!\")\n",
        "except:\n",
        "    print(\"\\n‚ö†Ô∏è  Not in Colab or Drive mounted\")\n",
        "\n",
        "# Create results directory with timestamp\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "RESULTS_DIR = os.path.join(RESULTS_DIR, f'test_run_{timestamp}')\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "print(f\"üìÅ Results will be saved to: {RESULTS_DIR}\")\n",
        "\n",
        "# ==================== MODEL CLASS ====================\n",
        "class DeBERTaEmotionClassifier(nn.Module):\n",
        "    \"\"\"DeBERTa v3 with classification head\"\"\"\n",
        "    def __init__(self, config, num_labels):\n",
        "        super(DeBERTaEmotionClassifier, self).__init__()\n",
        "        self.deberta = DebertaV2Model(config)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "        cls_output = sequence_output[:, 0, :]  # [CLS] token\n",
        "        cls_output = self.dropout(cls_output)\n",
        "        logits = self.classifier(cls_output)\n",
        "        return logits\n",
        "\n",
        "# ==================== LOAD METADATA ====================\n",
        "def load_metadata():\n",
        "    \"\"\"Load dataset metadata\"\"\"\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(\"STEP 1: LOADING METADATA\")\n",
        "    print(\"=\"*90)\n",
        "\n",
        "    try:\n",
        "        print(f\"üìÇ Loading from: {METADATA_PATH}\")\n",
        "        with open(METADATA_PATH, 'r') as f:\n",
        "            metadata = json.load(f)\n",
        "\n",
        "        emotion_classes = metadata['emotion_classes']\n",
        "        num_classes = metadata['num_classes']\n",
        "\n",
        "        print(f\"‚úÖ Metadata loaded!\")\n",
        "        print(f\"   Emotions: {', '.join(emotion_classes)}\")\n",
        "        print(f\"   Classes: {num_classes}\")\n",
        "\n",
        "        return metadata, emotion_classes, num_classes\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERROR loading metadata: {e}\")\n",
        "        raise\n",
        "\n",
        "# ==================== LOAD MODEL ====================\n",
        "def load_model(num_classes):\n",
        "    \"\"\"Load DeBERTa model with classifier from saved files\"\"\"\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(\"STEP 2: LOADING MODEL\")\n",
        "    print(\"=\"*90)\n",
        "\n",
        "    try:\n",
        "        # Check if all required files exist\n",
        "        config_path = os.path.join(MODEL_DIR, 'config.json')\n",
        "        safetensors_path = os.path.join(MODEL_DIR, 'model.safetensors')\n",
        "        classifier_path = os.path.join(MODEL_DIR, 'classifier.pt')\n",
        "\n",
        "        for path, name in [(config_path, 'config.json'),\n",
        "                           (safetensors_path, 'model.safetensors'),\n",
        "                           (classifier_path, 'classifier.pt')]:\n",
        "            if not os.path.exists(path):\n",
        "                raise FileNotFoundError(f\"{name} not found at {path}\")\n",
        "\n",
        "        print(f\"‚úÖ All required files found in: {MODEL_DIR}\")\n",
        "\n",
        "        # Load config\n",
        "        print(f\"\\nüìÇ Loading config...\")\n",
        "        with open(config_path, 'r') as f:\n",
        "            config_dict = json.load(f)\n",
        "        config = DebertaV2Config(**config_dict)\n",
        "        print(f\"‚úÖ Config loaded\")\n",
        "        print(f\"   Hidden size: {config.hidden_size}\")\n",
        "        print(f\"   Num layers: {config.num_hidden_layers}\")\n",
        "        print(f\"   Num heads: {config.num_attention_heads}\")\n",
        "\n",
        "        # Initialize model\n",
        "        print(f\"\\nü§ñ Initializing model architecture...\")\n",
        "        model = DeBERTaEmotionClassifier(config, num_classes)\n",
        "\n",
        "        # Load DeBERTa weights from safetensors\n",
        "        print(f\"\\nüìÇ Loading DeBERTa weights from safetensors...\")\n",
        "        state_dict = safetensors.torch.load_file(safetensors_path)\n",
        "\n",
        "        # Load weights into deberta module\n",
        "        model.deberta.load_state_dict(state_dict, strict=True)\n",
        "        print(f\"‚úÖ DeBERTa weights loaded successfully\")\n",
        "\n",
        "        # Load classifier weights\n",
        "        print(f\"\\nüìÇ Loading classifier head...\")\n",
        "        classifier_checkpoint = torch.load(classifier_path, map_location='cpu', weights_only=False)\n",
        "\n",
        "        # Load classifier and dropout states\n",
        "        model.classifier.load_state_dict(classifier_checkpoint['classifier_state_dict'])\n",
        "        model.dropout.load_state_dict(classifier_checkpoint['dropout_state_dict'])\n",
        "        print(f\"‚úÖ Classifier head loaded successfully\")\n",
        "\n",
        "        # Move to device and set to eval mode\n",
        "        model.to(DEVICE)\n",
        "        model.eval()\n",
        "\n",
        "        # Count parameters\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "        print(f\"\\n‚úÖ Model ready!\")\n",
        "        print(f\"   Total parameters: {total_params:,}\")\n",
        "        print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "        print(f\"   Model size: ~{total_params * 4 / 1024**2:.2f} MB\")\n",
        "        print(f\"   Device: {DEVICE}\")\n",
        "\n",
        "        # Load training metrics if available\n",
        "        metrics_path = os.path.join(MODEL_DIR, 'metrics.json')\n",
        "        training_metrics = None\n",
        "        if os.path.exists(metrics_path):\n",
        "            with open(metrics_path, 'r') as f:\n",
        "                training_metrics = json.load(f)\n",
        "            print(f\"\\nüìä Training metrics found:\")\n",
        "            if 'accuracy' in training_metrics:\n",
        "                print(f\"   Val Accuracy: {training_metrics['accuracy']:.4f}\")\n",
        "            if 'f1_macro' in training_metrics:\n",
        "                print(f\"   Val F1 Macro: {training_metrics['f1_macro']:.4f}\")\n",
        "\n",
        "        return model, training_metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå ERROR loading model: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "# ==================== LOAD TEST DATA ====================\n",
        "def load_test_data():\n",
        "    \"\"\"Load test dataset\"\"\"\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(\"STEP 3: LOADING TEST DATA\")\n",
        "    print(\"=\"*90)\n",
        "\n",
        "    try:\n",
        "        print(f\"üìÇ Loading from: {TEST_DATA_PATH}\")\n",
        "        test_data = torch.load(TEST_DATA_PATH, weights_only=False)\n",
        "\n",
        "        print(f\"‚úÖ Test data loaded!\")\n",
        "        print(f\"   Samples: {test_data['input_ids'].shape[0]:,}\")\n",
        "        print(f\"   Max length: {test_data['input_ids'].shape[1]}\")\n",
        "\n",
        "        # Create dataset\n",
        "        test_dataset = TensorDataset(\n",
        "            test_data['input_ids'],\n",
        "            test_data['attention_mask'],\n",
        "            test_data['labels']\n",
        "        )\n",
        "\n",
        "        # Create dataloader\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "            pin_memory=True,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ DataLoader created!\")\n",
        "        print(f\"   Batches: {len(test_loader):,}\")\n",
        "        print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "\n",
        "        return test_loader, test_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERROR loading test data: {e}\")\n",
        "        raise\n",
        "\n",
        "# ==================== COMPREHENSIVE EVALUATION ====================\n",
        "def evaluate_model(model, test_loader, emotion_classes):\n",
        "    \"\"\"Comprehensive model evaluation with all metrics\"\"\"\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(\"STEP 4: EVALUATING MODEL\")\n",
        "    print(\"=\"*90)\n",
        "\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "    total_loss = 0\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"\\nüîç Running inference on test set...\")\n",
        "    progress_bar = tqdm(test_loader, desc=\"Testing\", unit=\"batch\", colour=\"blue\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            try:\n",
        "                input_ids = batch[0].to(DEVICE)\n",
        "                attention_mask = batch[1].to(DEVICE)\n",
        "                labels = batch[2].to(DEVICE)\n",
        "\n",
        "                with torch.cuda.amp.autocast(enabled=USE_MIXED_PRECISION):\n",
        "                    logits = model(input_ids, attention_mask)\n",
        "                    loss = criterion(logits, labels)\n",
        "                    probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "                progress_bar.set_postfix({'loss': f'{total_loss / len(all_preds) * BATCH_SIZE:.4f}'})\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n‚ö†Ô∏è  Error during evaluation: {e}\")\n",
        "                continue\n",
        "\n",
        "    # Convert to numpy\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_probs = np.array(all_probs)\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "\n",
        "    print(f\"\\n‚úÖ Inference complete!\")\n",
        "    print(f\"   Total samples: {len(all_preds):,}\")\n",
        "    print(f\"   Average loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Calculate all metrics\n",
        "    print(\"\\nüìä Computing metrics...\")\n",
        "\n",
        "    # Basic metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    # F1 scores\n",
        "    f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    f1_weighted = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    f1_micro = f1_score(all_labels, all_preds, average='micro', zero_division=0)\n",
        "    f1_per_class = f1_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "\n",
        "    # Precision\n",
        "    precision_macro = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    precision_weighted = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    precision_micro = precision_score(all_labels, all_preds, average='micro', zero_division=0)\n",
        "    precision_per_class = precision_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "\n",
        "    # Recall\n",
        "    recall_macro = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    recall_weighted = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    recall_micro = recall_score(all_labels, all_preds, average='micro', zero_division=0)\n",
        "    recall_per_class = recall_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "\n",
        "    # Advanced metrics\n",
        "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
        "    kappa = cohen_kappa_score(all_labels, all_preds)\n",
        "\n",
        "    # AUC-ROC\n",
        "    try:\n",
        "        auc_roc_macro = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='macro')\n",
        "        auc_roc_weighted = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='weighted')\n",
        "        auc_roc_per_class = roc_auc_score(all_labels, all_probs, multi_class='ovr', average=None)\n",
        "    except:\n",
        "        auc_roc_macro = 0.0\n",
        "        auc_roc_weighted = 0.0\n",
        "        auc_roc_per_class = np.zeros(len(emotion_classes))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    # Classification report\n",
        "    class_report = classification_report(\n",
        "        all_labels, all_preds,\n",
        "        target_names=emotion_classes,\n",
        "        zero_division=0,\n",
        "        output_dict=True\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ All metrics computed!\")\n",
        "\n",
        "    return {\n",
        "        'loss': avg_loss,\n",
        "        'accuracy': accuracy,\n",
        "        'balanced_accuracy': balanced_acc,\n",
        "        'f1_macro': f1_macro,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'f1_micro': f1_micro,\n",
        "        'f1_per_class': f1_per_class,\n",
        "        'precision_macro': precision_macro,\n",
        "        'precision_weighted': precision_weighted,\n",
        "        'precision_micro': precision_micro,\n",
        "        'precision_per_class': precision_per_class,\n",
        "        'recall_macro': recall_macro,\n",
        "        'recall_weighted': recall_weighted,\n",
        "        'recall_micro': recall_micro,\n",
        "        'recall_per_class': recall_per_class,\n",
        "        'mcc': mcc,\n",
        "        'kappa': kappa,\n",
        "        'auc_roc_macro': auc_roc_macro,\n",
        "        'auc_roc_weighted': auc_roc_weighted,\n",
        "        'auc_roc_per_class': auc_roc_per_class,\n",
        "        'confusion_matrix': cm,\n",
        "        'confusion_matrix_normalized': cm_normalized,\n",
        "        'classification_report': class_report,\n",
        "        'predictions': all_preds,\n",
        "        'labels': all_labels,\n",
        "        'probabilities': all_probs\n",
        "    }\n",
        "\n",
        "# ==================== PRINT RESULTS ====================\n",
        "def print_results(results, emotion_classes):\n",
        "    \"\"\"Print comprehensive results\"\"\"\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(\"TEST RESULTS\")\n",
        "    print(\"=\"*90)\n",
        "\n",
        "    print(f\"\\nüìä Overall Metrics:\")\n",
        "    print(f\"   Loss:               {results['loss']:.4f}\")\n",
        "    print(f\"   Accuracy:           {results['accuracy']:.4f}\")\n",
        "    print(f\"   Balanced Accuracy:  {results['balanced_accuracy']:.4f}\")\n",
        "    print(f\"   F1 Macro:           {results['f1_macro']:.4f}\")\n",
        "    print(f\"   F1 Weighted:        {results['f1_weighted']:.4f}\")\n",
        "    print(f\"   F1 Micro:           {results['f1_micro']:.4f}\")\n",
        "    print(f\"   Precision Macro:    {results['precision_macro']:.4f}\")\n",
        "    print(f\"   Precision Weighted: {results['precision_weighted']:.4f}\")\n",
        "    print(f\"   Recall Macro:       {results['recall_macro']:.4f}\")\n",
        "    print(f\"   Recall Weighted:    {results['recall_weighted']:.4f}\")\n",
        "    print(f\"   MCC:                {results['mcc']:.4f}\")\n",
        "    print(f\"   Cohen's Kappa:      {results['kappa']:.4f}\")\n",
        "    print(f\"   AUC-ROC Macro:      {results['auc_roc_macro']:.4f}\")\n",
        "    print(f\"   AUC-ROC Weighted:   {results['auc_roc_weighted']:.4f}\")\n",
        "\n",
        "    print(f\"\\nüìä Per-Class Metrics:\")\n",
        "    print(f\"{'Emotion':<12} {'F1':>6} {'Precision':>10} {'Recall':>8} {'AUC-ROC':>8}\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, emotion in enumerate(emotion_classes):\n",
        "        print(f\"{emotion:<12} {results['f1_per_class'][i]:>6.4f} \"\n",
        "              f\"{results['precision_per_class'][i]:>10.4f} \"\n",
        "              f\"{results['recall_per_class'][i]:>8.4f} \"\n",
        "              f\"{results['auc_roc_per_class'][i]:>8.4f}\")\n",
        "\n",
        "    print(f\"\\nüìä Confusion Matrix:\")\n",
        "    print(results['confusion_matrix'])\n",
        "\n",
        "# ==================== SAVE RESULTS ====================\n",
        "def save_results(results, emotion_classes, training_metrics=None):\n",
        "    \"\"\"Save all results and create visualizations\"\"\"\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(\"STEP 5: SAVING RESULTS\")\n",
        "    print(\"=\"*90)\n",
        "\n",
        "    # Save metrics as JSON\n",
        "    metrics_dict = {\n",
        "        'test_metrics': {\n",
        "            'loss': float(results['loss']),\n",
        "            'accuracy': float(results['accuracy']),\n",
        "            'balanced_accuracy': float(results['balanced_accuracy']),\n",
        "            'f1_macro': float(results['f1_macro']),\n",
        "            'f1_weighted': float(results['f1_weighted']),\n",
        "            'f1_micro': float(results['f1_micro']),\n",
        "            'precision_macro': float(results['precision_macro']),\n",
        "            'precision_weighted': float(results['precision_weighted']),\n",
        "            'precision_micro': float(results['precision_micro']),\n",
        "            'recall_macro': float(results['recall_macro']),\n",
        "            'recall_weighted': float(results['recall_weighted']),\n",
        "            'recall_micro': float(results['recall_micro']),\n",
        "            'mcc': float(results['mcc']),\n",
        "            'kappa': float(results['kappa']),\n",
        "            'auc_roc_macro': float(results['auc_roc_macro']),\n",
        "            'auc_roc_weighted': float(results['auc_roc_weighted']),\n",
        "        },\n",
        "        'per_class_metrics': {\n",
        "            emotion: {\n",
        "                'f1': float(results['f1_per_class'][i]),\n",
        "                'precision': float(results['precision_per_class'][i]),\n",
        "                'recall': float(results['recall_per_class'][i]),\n",
        "                'auc_roc': float(results['auc_roc_per_class'][i])\n",
        "            }\n",
        "            for i, emotion in enumerate(emotion_classes)\n",
        "        },\n",
        "        'confusion_matrix': results['confusion_matrix'].tolist(),\n",
        "        'confusion_matrix_normalized': results['confusion_matrix_normalized'].tolist(),\n",
        "        'classification_report': results['classification_report'],\n",
        "        'training_metrics': training_metrics\n",
        "    }\n",
        "\n",
        "    metrics_path = os.path.join(RESULTS_DIR, 'test_metrics.json')\n",
        "    with open(metrics_path, 'w') as f:\n",
        "        json.dump(metrics_dict, f, indent=2)\n",
        "    print(f\"‚úÖ Metrics saved: {metrics_path}\")\n",
        "\n",
        "    # Save detailed CSV\n",
        "    csv_data = []\n",
        "    for i, emotion in enumerate(emotion_classes):\n",
        "        csv_data.append({\n",
        "            'emotion': emotion,\n",
        "            'f1_score': results['f1_per_class'][i],\n",
        "            'precision': results['precision_per_class'][i],\n",
        "            'recall': results['recall_per_class'][i],\n",
        "            'auc_roc': results['auc_roc_per_class'][i],\n",
        "            'support': results['confusion_matrix'][i].sum()\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(csv_data)\n",
        "    csv_path = os.path.join(RESULTS_DIR, 'per_class_metrics.csv')\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"‚úÖ CSV saved: {csv_path}\")\n",
        "\n",
        "    # Create visualizations\n",
        "    print(\"\\nüìä Creating visualizations...\")\n",
        "    create_visualizations(results, emotion_classes, RESULTS_DIR)\n",
        "    print(\"‚úÖ All visualizations saved!\")\n",
        "\n",
        "# ==================== CREATE VISUALIZATIONS ====================\n",
        "def create_visualizations(results, emotion_classes, save_dir):\n",
        "    \"\"\"Create comprehensive visualizations\"\"\"\n",
        "\n",
        "    # Figure 1: Confusion Matrices\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Raw confusion matrix\n",
        "    sns.heatmap(results['confusion_matrix'], annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=emotion_classes, yticklabels=emotion_classes, ax=axes[0])\n",
        "    axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_ylabel('True Label', fontsize=12)\n",
        "    axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
        "\n",
        "    # Normalized confusion matrix\n",
        "    sns.heatmap(results['confusion_matrix_normalized'], annot=True, fmt='.2%', cmap='Blues',\n",
        "                xticklabels=emotion_classes, yticklabels=emotion_classes, ax=axes[1])\n",
        "    axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_ylabel('True Label', fontsize=12)\n",
        "    axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'confusion_matrices.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Figure 2: Per-class metrics\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    x = np.arange(len(emotion_classes))\n",
        "    width = 0.2\n",
        "\n",
        "    ax.bar(x - 1.5*width, results['f1_per_class'], width, label='F1 Score', alpha=0.8)\n",
        "    ax.bar(x - 0.5*width, results['precision_per_class'], width, label='Precision', alpha=0.8)\n",
        "    ax.bar(x + 0.5*width, results['recall_per_class'], width, label='Recall', alpha=0.8)\n",
        "    ax.bar(x + 1.5*width, results['auc_roc_per_class'], width, label='AUC-ROC', alpha=0.8)\n",
        "\n",
        "    ax.set_xlabel('Emotion', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Per-Class Metrics Comparison', fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(emotion_classes, rotation=45, ha='right')\n",
        "    ax.legend(loc='lower right')\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    ax.set_ylim([0, 1.05])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'per_class_metrics.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Figure 3: Overall metrics\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    metrics = ['Accuracy', 'Balanced\\nAccuracy', 'F1\\nMacro', 'F1\\nWeighted',\n",
        "               'Precision\\nMacro', 'Recall\\nMacro', 'MCC', 'Kappa', 'AUC-ROC\\nMacro']\n",
        "    values = [results['accuracy'], results['balanced_accuracy'], results['f1_macro'],\n",
        "              results['f1_weighted'], results['precision_macro'], results['recall_macro'],\n",
        "              results['mcc'], results['kappa'], results['auc_roc_macro']]\n",
        "\n",
        "    bars = ax.bar(metrics, values, color='skyblue', alpha=0.8, edgecolor='navy')\n",
        "    ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Overall Model Performance', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylim([0, 1.05])\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for bar, value in zip(bars, values):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{value:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'overall_metrics.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Figure 4: ROC Curves (one vs rest for each class)\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    labels_bin = np.eye(len(emotion_classes))[results['labels']]\n",
        "\n",
        "    for i, emotion in enumerate(emotion_classes):\n",
        "        fpr, tpr, _ = roc_curve(labels_bin[:, i], results['probabilities'][:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        axes[i].plot(fpr, tpr, color='darkorange', lw=2,\n",
        "                     label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "        axes[i].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
        "        axes[i].set_xlim([0.0, 1.0])\n",
        "        axes[i].set_ylim([0.0, 1.05])\n",
        "        axes[i].set_xlabel('False Positive Rate')\n",
        "        axes[i].set_ylabel('True Positive Rate')\n",
        "        axes[i].set_title(f'ROC - {emotion}')\n",
        "        axes[i].legend(loc=\"lower right\")\n",
        "        axes[i].grid(alpha=0.3)\n",
        "\n",
        "    # Hide the last subplot if odd number of classes\n",
        "    if len(emotion_classes) < 8:\n",
        "        axes[-1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'roc_curves.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Figure 5: Precision-Recall Curves\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for i, emotion in enumerate(emotion_classes):\n",
        "        precision, recall, _ = precision_recall_curve(labels_bin[:, i], results['probabilities'][:, i])\n",
        "        avg_precision = average_precision_score(labels_bin[:, i], results['probabilities'][:, i])\n",
        "\n",
        "        axes[i].plot(recall, precision, color='darkblue', lw=2,\n",
        "                     label=f'AP = {avg_precision:.3f}')\n",
        "        axes[i].set_xlim([0.0, 1.0])\n",
        "        axes[i].set_ylim([0.0, 1.05])\n",
        "        axes[i].set_xlabel('Recall')\n",
        "        axes[i].set_ylabel('Precision')\n",
        "        axes[i].set_title(f'Precision-Recall - {emotion}')\n",
        "        axes[i].legend(loc=\"lower left\")\n",
        "        axes[i].grid(alpha=0.3)\n",
        "\n",
        "    # Hide the last subplot if odd number of classes\n",
        "    if len(emotion_classes) < 8:\n",
        "        axes[-1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'precision_recall_curves.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Figure 6: Error Analysis - Misclassification patterns\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "    # Create a matrix showing where errors occur\n",
        "    error_matrix = results['confusion_matrix'].copy()\n",
        "    np.fill_diagonal(error_matrix, 0)  # Remove correct predictions\n",
        "\n",
        "    sns.heatmap(error_matrix, annot=True, fmt='d', cmap='Reds',\n",
        "                xticklabels=emotion_classes, yticklabels=emotion_classes, ax=ax)\n",
        "    ax.set_title('Misclassification Pattern (Errors Only)', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('True Label', fontsize=12)\n",
        "    ax.set_xlabel('Predicted Label', fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'error_analysis.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Figure 7: Class-wise accuracy\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    class_accuracies = np.diag(results['confusion_matrix_normalized'])\n",
        "    colors = ['green' if acc > 0.9 else 'orange' if acc > 0.8 else 'red' for acc in class_accuracies]\n",
        "\n",
        "    bars = ax.barh(emotion_classes, class_accuracies, color=colors, alpha=0.7)\n",
        "    ax.set_xlabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Per-Class Accuracy', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlim([0, 1.0])\n",
        "    ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for i, (bar, acc) in enumerate(zip(bars, class_accuracies)):\n",
        "        ax.text(acc + 0.01, bar.get_y() + bar.get_height()/2,\n",
        "                f'{acc:.3f}', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'class_accuracies.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Figure 8: Comparison with training metrics (if available)\n",
        "    # This will be handled in the main function if training metrics exist\n",
        "\n",
        "# ==================== CREATE COMPARISON PLOT ====================\n",
        "def create_train_test_comparison(test_results, training_metrics, emotion_classes, save_dir):\n",
        "    \"\"\"Create comparison between training validation and test results\"\"\"\n",
        "    if training_metrics is None:\n",
        "        return\n",
        "\n",
        "    print(\"\\nüìä Creating train/test comparison...\")\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # Overall metrics comparison\n",
        "    ax = axes[0, 0]\n",
        "    metrics = ['Accuracy', 'F1 Macro', 'F1 Weighted', 'Precision\\nMacro', 'Recall\\nMacro']\n",
        "\n",
        "    val_values = [\n",
        "        training_metrics.get('accuracy', 0),\n",
        "        training_metrics.get('f1_macro', 0),\n",
        "        training_metrics.get('f1_weighted', 0),\n",
        "        training_metrics.get('precision_macro', 0),\n",
        "        training_metrics.get('recall_macro', 0)\n",
        "    ]\n",
        "\n",
        "    test_values = [\n",
        "        test_results['accuracy'],\n",
        "        test_results['f1_macro'],\n",
        "        test_results['f1_weighted'],\n",
        "        test_results['precision_macro'],\n",
        "        test_results['recall_macro']\n",
        "    ]\n",
        "\n",
        "    x = np.arange(len(metrics))\n",
        "    width = 0.35\n",
        "\n",
        "    ax.bar(x - width/2, val_values, width, label='Validation', alpha=0.8)\n",
        "    ax.bar(x + width/2, test_values, width, label='Test', alpha=0.8)\n",
        "\n",
        "    ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Validation vs Test Performance', fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(metrics, rotation=45, ha='right')\n",
        "    ax.legend()\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    ax.set_ylim([0, 1.05])\n",
        "\n",
        "    # Per-class F1 comparison\n",
        "    ax = axes[0, 1]\n",
        "\n",
        "    val_f1 = training_metrics.get('f1_per_class', [0] * len(emotion_classes))\n",
        "    test_f1 = test_results['f1_per_class']\n",
        "\n",
        "    x = np.arange(len(emotion_classes))\n",
        "    width = 0.35\n",
        "\n",
        "    ax.bar(x - width/2, val_f1, width, label='Validation', alpha=0.8)\n",
        "    ax.bar(x + width/2, test_f1, width, label='Test', alpha=0.8)\n",
        "\n",
        "    ax.set_ylabel('F1 Score', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Per-Class F1: Validation vs Test', fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(emotion_classes, rotation=45, ha='right')\n",
        "    ax.legend()\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    ax.set_ylim([0, 1.05])\n",
        "\n",
        "    # Performance difference\n",
        "    ax = axes[1, 0]\n",
        "\n",
        "    diff = np.array(test_f1) - np.array(val_f1)\n",
        "    colors = ['green' if d >= 0 else 'red' for d in diff]\n",
        "\n",
        "    bars = ax.bar(emotion_classes, diff, color=colors, alpha=0.7)\n",
        "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "    ax.set_ylabel('F1 Difference (Test - Val)', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Performance Change: Validation to Test', fontsize=14, fontweight='bold')\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "\n",
        "    # Add value labels\n",
        "    for bar, value in zip(bars, diff):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{value:+.3f}', ha='center',\n",
        "                va='bottom' if height > 0 else 'top', fontsize=9, fontweight='bold')\n",
        "\n",
        "    # Summary text\n",
        "    ax = axes[1, 1]\n",
        "    ax.axis('off')\n",
        "\n",
        "    summary_text = f\"\"\"\n",
        "    VALIDATION vs TEST SUMMARY\n",
        "    {'='*40}\n",
        "\n",
        "    Overall Metrics:\n",
        "    ‚Ä¢ Accuracy:     Val={val_values[0]:.4f}  Test={test_values[0]:.4f}  Œî={test_values[0]-val_values[0]:+.4f}\n",
        "    ‚Ä¢ F1 Macro:     Val={val_values[1]:.4f}  Test={test_values[1]:.4f}  Œî={test_values[1]-val_values[1]:+.4f}\n",
        "    ‚Ä¢ F1 Weighted:  Val={val_values[2]:.4f}  Test={test_values[2]:.4f}  Œî={test_values[2]-val_values[2]:+.4f}\n",
        "\n",
        "    Best Performing Classes (Test):\n",
        "    \"\"\"\n",
        "\n",
        "    # Add top 3 classes\n",
        "    top_3_idx = np.argsort(test_f1)[-3:][::-1]\n",
        "    for idx in top_3_idx:\n",
        "        summary_text += f\"    ‚Ä¢ {emotion_classes[idx]}: {test_f1[idx]:.4f}\\n\"\n",
        "\n",
        "    summary_text += \"\\n    Most Improved Classes (Val‚ÜíTest):\\n\"\n",
        "    top_improved = np.argsort(diff)[-3:][::-1]\n",
        "    for idx in top_improved:\n",
        "        if diff[idx] > 0:\n",
        "            summary_text += f\"    ‚Ä¢ {emotion_classes[idx]}: {diff[idx]:+.4f}\\n\"\n",
        "\n",
        "    ax.text(0.1, 0.9, summary_text, fontsize=11, family='monospace',\n",
        "            verticalalignment='top', transform=ax.transAxes)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'train_test_comparison.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"‚úÖ Comparison plot saved!\")\n",
        "\n",
        "# ==================== CREATE SUMMARY REPORT ====================\n",
        "def create_summary_report(results, emotion_classes, training_metrics, save_dir):\n",
        "    \"\"\"Create a comprehensive text summary report\"\"\"\n",
        "\n",
        "    report_path = os.path.join(save_dir, 'test_report.txt')\n",
        "\n",
        "    with open(report_path, 'w') as f:\n",
        "        f.write(\"=\"*90 + \"\\n\")\n",
        "        f.write(\" \"*20 + \"DeBERTa v3 MODEL TEST REPORT\\n\")\n",
        "        f.write(\" \"*25 + \"SuperEmotion Dataset\\n\")\n",
        "        f.write(\"=\"*90 + \"\\n\\n\")\n",
        "\n",
        "        f.write(f\"Test Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"Model: DeBERTa v3 Base + Classification Head\\n\")\n",
        "        f.write(f\"Dataset: 7 Emotion Classes\\n\")\n",
        "        f.write(f\"Test Samples: {len(results['labels']):,}\\n\\n\")\n",
        "\n",
        "        f.write(\"=\"*90 + \"\\n\")\n",
        "        f.write(\"OVERALL PERFORMANCE\\n\")\n",
        "        f.write(\"=\"*90 + \"\\n\\n\")\n",
        "\n",
        "        f.write(f\"Loss:                 {results['loss']:.6f}\\n\")\n",
        "        f.write(f\"Accuracy:             {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)\\n\")\n",
        "        f.write(f\"Balanced Accuracy:    {results['balanced_accuracy']:.4f} ({results['balanced_accuracy']*100:.2f}%)\\n\\n\")\n",
        "\n",
        "        f.write(\"F1 Scores:\\n\")\n",
        "        f.write(f\"  ‚Ä¢ Macro:            {results['f1_macro']:.4f}\\n\")\n",
        "        f.write(f\"  ‚Ä¢ Weighted:         {results['f1_weighted']:.4f}\\n\")\n",
        "        f.write(f\"  ‚Ä¢ Micro:            {results['f1_micro']:.4f}\\n\\n\")\n",
        "\n",
        "        f.write(\"Precision:\\n\")\n",
        "        f.write(f\"  ‚Ä¢ Macro:            {results['precision_macro']:.4f}\\n\")\n",
        "        f.write(f\"  ‚Ä¢ Weighted:         {results['precision_weighted']:.4f}\\n\")\n",
        "        f.write(f\"  ‚Ä¢ Micro:            {results['precision_micro']:.4f}\\n\\n\")\n",
        "\n",
        "        f.write(\"Recall:\\n\")\n",
        "        f.write(f\"  ‚Ä¢ Macro:            {results['recall_macro']:.4f}\\n\")\n",
        "        f.write(f\"  ‚Ä¢ Weighted:         {results['recall_weighted']:.4f}\\n\")\n",
        "        f.write(f\"  ‚Ä¢ Micro:            {results['recall_micro']:.4f}\\n\\n\")\n",
        "\n",
        "        f.write(\"Advanced Metrics:\\n\")\n",
        "        f.write(f\"  ‚Ä¢ Matthews Correlation Coefficient: {results['mcc']:.4f}\\n\")\n",
        "        f.write(f\"  ‚Ä¢ Cohen's Kappa:                    {results['kappa']:.4f}\\n\")\n",
        "        f.write(f\"  ‚Ä¢ AUC-ROC (Macro):                  {results['auc_roc_macro']:.4f}\\n\")\n",
        "        f.write(f\"  ‚Ä¢ AUC-ROC (Weighted):               {results['auc_roc_weighted']:.4f}\\n\\n\")\n",
        "\n",
        "        f.write(\"=\"*90 + \"\\n\")\n",
        "        f.write(\"PER-CLASS PERFORMANCE\\n\")\n",
        "        f.write(\"=\"*90 + \"\\n\\n\")\n",
        "\n",
        "        f.write(f\"{'Emotion':<12} {'F1':>8} {'Precision':>10} {'Recall':>8} {'AUC-ROC':>8} {'Support':>8}\\n\")\n",
        "        f.write(\"-\"*70 + \"\\n\")\n",
        "\n",
        "        for i, emotion in enumerate(emotion_classes):\n",
        "            support = results['confusion_matrix'][i].sum()\n",
        "            f.write(f\"{emotion:<12} \"\n",
        "                   f\"{results['f1_per_class'][i]:>8.4f} \"\n",
        "                   f\"{results['precision_per_class'][i]:>10.4f} \"\n",
        "                   f\"{results['recall_per_class'][i]:>8.4f} \"\n",
        "                   f\"{results['auc_roc_per_class'][i]:>8.4f} \"\n",
        "                   f\"{support:>8}\\n\")\n",
        "\n",
        "        f.write(\"\\n\" + \"=\"*90 + \"\\n\")\n",
        "        f.write(\"BEST & WORST PERFORMING CLASSES\\n\")\n",
        "        f.write(\"=\"*90 + \"\\n\\n\")\n",
        "\n",
        "        # Best performing\n",
        "        best_idx = np.argsort(results['f1_per_class'])[-3:][::-1]\n",
        "        f.write(\"Top 3 Classes by F1 Score:\\n\")\n",
        "        for rank, idx in enumerate(best_idx, 1):\n",
        "            f.write(f\"  {rank}. {emotion_classes[idx]:<12} F1={results['f1_per_class'][idx]:.4f}\\n\")\n",
        "\n",
        "        # Worst performing\n",
        "        worst_idx = np.argsort(results['f1_per_class'])[:3]\n",
        "        f.write(\"\\nBottom 3 Classes by F1 Score:\\n\")\n",
        "        for rank, idx in enumerate(worst_idx, 1):\n",
        "            f.write(f\"  {rank}. {emotion_classes[idx]:<12} F1={results['f1_per_class'][idx]:.4f}\\n\")\n",
        "\n",
        "        f.write(\"\\n\" + \"=\"*90 + \"\\n\")\n",
        "        f.write(\"CONFUSION MATRIX\\n\")\n",
        "        f.write(\"=\"*90 + \"\\n\\n\")\n",
        "\n",
        "        # Print confusion matrix\n",
        "        f.write(\"Raw Counts:\\n\")\n",
        "        f.write(f\"{'':>12} \")\n",
        "        for emotion in emotion_classes:\n",
        "            f.write(f\"{emotion[:8]:>8} \")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "        for i, emotion in enumerate(emotion_classes):\n",
        "            f.write(f\"{emotion[:12]:>12} \")\n",
        "            for j in range(len(emotion_classes)):\n",
        "                f.write(f\"{results['confusion_matrix'][i][j]:>8} \")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "        f.write(\"\\nNormalized (%):\\n\")\n",
        "        f.write(f\"{'':>12} \")\n",
        "        for emotion in emotion_classes:\n",
        "            f.write(f\"{emotion[:8]:>8} \")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "        for i, emotion in enumerate(emotion_classes):\n",
        "            f.write(f\"{emotion[:12]:>12} \")\n",
        "            for j in range(len(emotion_classes)):\n",
        "                f.write(f\"{results['confusion_matrix_normalized'][i][j]*100:>7.1f}% \")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "        # Comparison with validation if available\n",
        "        if training_metrics:\n",
        "            f.write(\"\\n\" + \"=\"*90 + \"\\n\")\n",
        "            f.write(\"VALIDATION vs TEST COMPARISON\\n\")\n",
        "            f.write(\"=\"*90 + \"\\n\\n\")\n",
        "\n",
        "            val_acc = training_metrics.get('accuracy', 0)\n",
        "            val_f1 = training_metrics.get('f1_macro', 0)\n",
        "\n",
        "            f.write(f\"Accuracy:     Val={val_acc:.4f}  Test={results['accuracy']:.4f}  \"\n",
        "                   f\"Œî={results['accuracy']-val_acc:+.4f}\\n\")\n",
        "            f.write(f\"F1 Macro:     Val={val_f1:.4f}  Test={results['f1_macro']:.4f}  \"\n",
        "                   f\"Œî={results['f1_macro']-val_f1:+.4f}\\n\\n\")\n",
        "\n",
        "            if 'f1_per_class' in training_metrics:\n",
        "                val_f1_per_class = training_metrics['f1_per_class']\n",
        "                f.write(\"Per-Class F1 Changes:\\n\")\n",
        "                for i, emotion in enumerate(emotion_classes):\n",
        "                    diff = results['f1_per_class'][i] - val_f1_per_class[i]\n",
        "                    symbol = \"‚Üë\" if diff > 0 else \"‚Üì\" if diff < 0 else \"=\"\n",
        "                    f.write(f\"  {emotion:<12} Val={val_f1_per_class[i]:.4f}  \"\n",
        "                           f\"Test={results['f1_per_class'][i]:.4f}  {symbol} {abs(diff):.4f}\\n\")\n",
        "\n",
        "        f.write(\"\\n\" + \"=\"*90 + \"\\n\")\n",
        "        f.write(\"ERROR ANALYSIS\\n\")\n",
        "        f.write(\"=\"*90 + \"\\n\\n\")\n",
        "\n",
        "        # Most common misclassifications\n",
        "        error_matrix = results['confusion_matrix'].copy()\n",
        "        np.fill_diagonal(error_matrix, 0)\n",
        "\n",
        "        f.write(\"Top 5 Most Common Misclassifications:\\n\")\n",
        "        flat_errors = []\n",
        "        for i in range(len(emotion_classes)):\n",
        "            for j in range(len(emotion_classes)):\n",
        "                if i != j and error_matrix[i][j] > 0:\n",
        "                    flat_errors.append((error_matrix[i][j], i, j))\n",
        "\n",
        "        flat_errors.sort(reverse=True)\n",
        "        for rank, (count, true_idx, pred_idx) in enumerate(flat_errors[:5], 1):\n",
        "            total = results['confusion_matrix'][true_idx].sum()\n",
        "            pct = (count / total) * 100\n",
        "            f.write(f\"  {rank}. {emotion_classes[true_idx]} ‚Üí {emotion_classes[pred_idx]}: \"\n",
        "                   f\"{count} errors ({pct:.1f}%)\\n\")\n",
        "\n",
        "        f.write(\"\\n\" + \"=\"*90 + \"\\n\")\n",
        "        f.write(\"END OF REPORT\\n\")\n",
        "        f.write(\"=\"*90 + \"\\n\")\n",
        "\n",
        "    print(f\"‚úÖ Summary report saved: {report_path}\")\n",
        "\n",
        "# ==================== MAIN ====================\n",
        "def main():\n",
        "    \"\"\"Main evaluation pipeline\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(\"üöÄ STARTING COMPREHENSIVE MODEL TESTING\")\n",
        "    print(\"=\"*90)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        # Load metadata\n",
        "        metadata, emotion_classes, num_classes = load_metadata()\n",
        "\n",
        "        # Load model\n",
        "        model, training_metrics = load_model(num_classes)\n",
        "\n",
        "        # Load test data\n",
        "        test_loader, test_data = load_test_data()\n",
        "\n",
        "        # Evaluate model\n",
        "        results = evaluate_model(model, test_loader, emotion_classes)\n",
        "\n",
        "        # Print results to console\n",
        "        print_results(results, emotion_classes)\n",
        "\n",
        "        # Save all results\n",
        "        save_results(results, emotion_classes, training_metrics)\n",
        "\n",
        "        # Create comparison with training if available\n",
        "        if training_metrics:\n",
        "            create_train_test_comparison(results, training_metrics, emotion_classes, RESULTS_DIR)\n",
        "\n",
        "        # Create summary report\n",
        "        create_summary_report(results, emotion_classes, training_metrics, RESULTS_DIR)\n",
        "\n",
        "        # Save predictions for further analysis\n",
        "        predictions_df = pd.DataFrame({\n",
        "            'true_label': [emotion_classes[i] for i in results['labels']],\n",
        "            'predicted_label': [emotion_classes[i] for i in results['predictions']],\n",
        "            'true_label_id': results['labels'],\n",
        "            'predicted_label_id': results['predictions'],\n",
        "            'correct': results['labels'] == results['predictions']\n",
        "        })\n",
        "\n",
        "        # Add probability for each class\n",
        "        for i, emotion in enumerate(emotion_classes):\n",
        "            predictions_df[f'prob_{emotion}'] = results['probabilities'][:, i]\n",
        "\n",
        "        predictions_path = os.path.join(RESULTS_DIR, 'predictions.csv')\n",
        "        predictions_df.to_csv(predictions_path, index=False)\n",
        "        print(f\"\\n‚úÖ Predictions saved: {predictions_path}\")\n",
        "\n",
        "        # Calculate and print execution time\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        print(\"\\n\" + \"=\"*90)\n",
        "        print(\"üéâ TESTING COMPLETE!\")\n",
        "        print(\"=\"*90)\n",
        "        print(f\"\\n‚è±Ô∏è  Total time: {total_time:.2f}s ({total_time/60:.2f} minutes)\")\n",
        "        print(f\"\\nüìä Key Results:\")\n",
        "        print(f\"   ‚Ä¢ Accuracy: {results['accuracy']:.4f}\")\n",
        "        print(f\"   ‚Ä¢ F1 Macro: {results['f1_macro']:.4f}\")\n",
        "        print(f\"   ‚Ä¢ AUC-ROC:  {results['auc_roc_macro']:.4f}\")\n",
        "        print(f\"\\nüìÅ All results saved to: {RESULTS_DIR}\")\n",
        "        print(f\"\\nüìÑ Generated files:\")\n",
        "        print(f\"   ‚Ä¢ test_metrics.json - All metrics in JSON format\")\n",
        "        print(f\"   ‚Ä¢ per_class_metrics.csv - Detailed per-class metrics\")\n",
        "        print(f\"   ‚Ä¢ predictions.csv - All predictions with probabilities\")\n",
        "        print(f\"   ‚Ä¢ test_report.txt - Comprehensive text report\")\n",
        "        print(f\"   ‚Ä¢ confusion_matrices.png - Raw and normalized confusion matrices\")\n",
        "        print(f\"   ‚Ä¢ per_class_metrics.png - Per-class performance visualization\")\n",
        "        print(f\"   ‚Ä¢ overall_metrics.png - Overall model performance\")\n",
        "        print(f\"   ‚Ä¢ roc_curves.png - ROC curves for each class\")\n",
        "        print(f\"   ‚Ä¢ precision_recall_curves.png - PR curves for each class\")\n",
        "        print(f\"   ‚Ä¢ error_analysis.png - Misclassification patterns\")\n",
        "        print(f\"   ‚Ä¢ class_accuracies.png - Per-class accuracy breakdown\")\n",
        "        if training_metrics:\n",
        "            print(f\"   ‚Ä¢ train_test_comparison.png - Validation vs Test comparison\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*90)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå CRITICAL ERROR: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IUIMevFnrZi3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}